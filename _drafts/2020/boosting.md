---
layout:     post
title:      Boosting
date:       2020-07-22
tags: [boost, adaboost]
categories: 
- machine learning
---

对于分类任务，提升算法(boosting)通过改变训练样本的权重，训练多个分类器，并将这个分类器进行线性组合，提高分类的性能。其思想是，对于复杂任务，汇总多个专家的判断将好于任意一个专家单独的判断。

在历史上有"强可学习"与"弱可学习"两个概念:

* 如果一个概念在概率近似正确(probably approximately correct, PAC)学习的框架下，存在一个多项式算法能够学习它，并且准确率很高，则称其为强可学习
* 反之，如果一个多项式算法学习的准确率仅比随机猜测稍好，则称其弱可学习

二者是等价的，即一个概念强可学习当且仅当其弱可学习。而弱学习算法的发现要简单容易的多，一个问题是：能否将弱学习算法提升(boost)为强学习算法。方法代表:Adaboost. 

多数提升方法都是 **训练数据的概率分布(权值分布)，针对不同的训练数据分布调用弱学习算法学习一系列弱分类器**。因此 Boost 需要解决两个问题:
1. 如何改变训练数据的概率分布 ?
2. 如何将弱分类器组合成强分类器 ？

Adaboost 采用的方法
1. 提高被错误分类的样本的权值
2. 加权多数表决。加大分类误差率小的分类器的权重，减小误差率大的分类器的权重。 

# Adaboost
特点，通过迭代每次学习一个基本分类器。每次迭代中，提高前一轮中被错误分类的样本的权重，减小被正确分类的样本的权重。最终，线性组合基本分类器得到一个强分类器。其中，对分类误差率较大的分类器赋予较小的权值，误差率较小的赋予较大的权值。

最基本的性质是它能在学习过程中不断减少训练误差。

[More on Adaboost](https://gaoangliu.github.io/archives/AdaBoost-Algorithm.html).

