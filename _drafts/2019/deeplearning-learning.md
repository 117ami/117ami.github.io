---
layout:     post
title:      Learn Deep Learning
date:       2019-09-25
img:  realdeeplearning.png
tags: [Deep Learning, AI]
categories: 
- deep learning
- AI 
---

20世纪40~60年代，控制论(cybernetics)。随着生物学习理论的发展与第一个模型的实现(如感知机1958)，能实现单个神经元的训练。

<!--break-->

# 历史

## 三次浪潮
20世纪40~60年代，控制论(cybernetics)。随着生物学习理论的发展与第一个模型的实现(如感知机1958)，能实现单个神经元的训练。

### 联结主义
* 20世纪80~90年代(1980~1995)，联结主义(connectionism)，其中心思想是，当网络将大量简单的计算单元连接在一起时可能实现智能行为。
* 联结主义是在认知科学的背景下出现的，20世纪80年代初期，大多数认知科学家研究符号推理模型，但很难解释大脑如何真正使用神经元实现推理功能。
* 成就
    - 反向传播在训练具有内部表示的深度神经网络中的成功使用，以及反向传播算法的普及
    - LSTM(long short-term memory)的引入(Hochreiter & Schmidhuber,1997)

尽管这个阶段提出的很多算法在今天都表现的很好，神经网络研究的浪潮还是在20世纪90年代中期逐渐衰退，总结下来，至少归因于以下几点:
1. 计算机软硬件基础设施发展滞后，以当时计算机硬件的水准，很难训练出足够水平的神经网络。
2. 基于神经网络的创业公司野心勃勃但不切实际，令投资者失望
3. 机器学习的其他领域，比如核方法(Boser et al. 1992)和图模型(Jordan, 1998)取得进步，并在很多重要任务上实现了很好的效果。

2006~now，深度学习的复兴

## Deep Learning v.s. AI

许多AI任务可以通过**提取一个合适特征，然后将这些特征提供给简单的机器学习算法**来解决问题。 但提取哪些特征是一个难题，解决这个问题的途径之一是**使用机器学习来发掘表示本身**，这种方法称为：表示学习(representation learning)。

典型例子：自编码器(autoencoder)。学习到的表示往往比手动设计的表示表现的要好，且只需极少人工干预。

表示学习的一个困难在于：多个变差因素同时影响着我们能够观察到的每一个数据，从原始数据是抽取高层次、抽象的特征非常困难。深度学习通过其他简单的表示来表达复杂表示，这解决了表示学习的核心问题。
典型例子：前馈深度网络。 

总：DL是ML的一种，是一种能够使用计算机系统从数据和经验中得到提升的技术。

<p align="center"> 
    <img src="{{site.baseurl}}/images/dl.vs.ai.png" width="500px">
</p>


<!-- Math -->
# Basic Math

## 线性相关与生成子空间
<p align="center">  $$ Ax = b  $$ </p>
其中 $$A \in \mathrm{R}^{m \times n}$$ matrix, $$b \in \mathrm{R}^n $$ vector。对于上面的方程组来说，要么不存在解，要么存在唯一解或者无穷解，不可能存在大于1但小于无穷个解的情况 (不然，两个解的线性组合 $$\alpha x + (1-\alpha)y$$也是方程组的解)。

### 方程在每一点存在解的必要条件 $$ n \geq m$$

$$Ax = \Sigma_{i\in[1,n]} x_i A_{:, i} = \Sigma_i c_i v^{(i)}$$, where $$v^{(i)}$$ 是$$A$$的列向量。 判定以上方程组是否存在解，即判定$$b$$是否在$$A$$的生成子空间中。 这个特殊的生成子空间称为$$A$$的**列空间**或**值域(range)** . 

因为 $$ b \in \mathrm{R^m} $$，如果 $$\mathrm{R}^m$$ 中一个点不在 $$A$$ 的列空间中，那该点对应的 $$b$$ 没有解，因此 $$A$$ 至少有 $$m$$ 列，即 $$n \geq m$$。 举例，$$ m = 3, n = 2 $$，那么无论 $$x$$ 如何变化，它只能将 $$A$$ 映射到 $$\mathrm{R}^3 $$的一个平面，只有当 $$b$$ 处于这个平面时， 方程才有解。 

### 存在解的充分条件
$$ n \geq m$$ 并不能保证方程一定存在解，因为列向量可能**线性相关**，即某一个向量可能表示为其他一组向量的线性组合。要使其列空间涵盖整个 $$\mathrm{R}^m$$， 需要满足什么条件 ？

Ans: 矩阵必须包含**至少一组$$m$$个线性无关**的向量。

<!-- > 方程 $$Ax=b$$ 对每一个$$b$$都有解的充分必要条件：向量集恰好有$$m$$个线性不相关的向量，而非至少有$$m$$个。 二者还有有区别的。因为至少有$$m$$个包含了 -->

但要使矩阵可逆，需要保证对每一个 $$b$$ 至多只有一个解，因此要保证矩阵至多只有 $$m$$ 个列向量，即必须是一个方阵(square)，且所有列向量线性无关。这样的矩阵称为 **奇异矩阵**。

### 特征分解 
矩阵 $$A$$ 的**特征向量(eigenvector)**是指与 $$A$$ 相乘后相当于对该向量进行缩放的非零向量 $$v$$:  $$Av = \lambda v$$ ，
其中标题 $$\lambda$$ 称为这个特征向量对应的**特征值(eigenvalue)**. 

* 如果 $$v$$ 是 $$A$$ 的特征向量，那么任意缩放后的向量 $$sv(s \in \mathrm{R}, s \neq 0)$$ 也是 $$A$$ 的特征向量
* 假设 $$A$$ 有 $$n$$ 个线性无关的特征向量  $$\{v_1, ..., v_n\}$$ ，对应的特征值  $$\{\lambda_1, ..., \lambda_n\}$$ 。将这些特征向量连接成一个矩阵  $$V = \{v^{(1)}, ..., v^{(n)}\}$$ ，那么有 $$AV = V \text{diag}(\lambda) ( \text{ where }\lambda = [\lambda_1, ..., \lambda_n ]^T) $$，进一步地，
<p>  $$A = V \text{diag}(\lambda) V^{-1}$$  </p>
其中

### 奇异值分解
将矩阵分解为奇异向量(singular vector)和奇异值(singular value)的方法称为奇异值分解(singular value decomposition SVD).
每个实数矩阵都有一个SVD，但不一定有特征分解，e.g., 非方阵的矩阵没有特征分解。

# 概率与信息论
概率率是**表示不确定性声明的数学框架**。它提供了量化不确定性的方法，也提供也用于推导新的不确定性声明的公理 。

对于AI系统有两个方面的用途：
1. 概率法则告诉我们AI系统如何推理，据此我们可以设计一些算法或者估算由概率论导出来的表达式
2. 可以用概率与统计从理论上分析我们提出的AI系统的行为

不确定性的来源：
1. 被建模系统内在的随机性
2. 不完全观测 
3. 不完全建模, e.g., 用离散化的空间建模连续空间

## 概率分布
Probability distribution 描述了随机变量或者一簇随机变量在每一个可能取到的状态的可能性大小。

### 离散型变量 
可以**概率质量函数(probability mass function PMF)**描述。举例
* Bernoulli分布，单个二值随机变量分布：  $$x \sim P(x), P(x) = \phi^x(1-\phi)^{(1-x)}$$ ，其中 $$\phi$$  给出了随机变量 $$x$$ 等于1的概率

### 连续性变量 
可以用**概率密码函数(probability density function)**刻画。

### 常用概率分布
**正态分布(normal distribution)**
<p>  $$\mathcal{N}(x; \mu, \sigma^2) = \sqrt{\frac{1}{2\pi\sigma^2}}\text{exp}(- \frac{1}{2\sigma^2}(x - \mu)^2)$$  </p>
ND由两个参数决定  $$\mu \in \mathrm{R}, \sigma \in (0, \infty)$$ , 前者给出中心峰值的座标，即分布的均值； 后者是分布的标准差，控制分布的宽度。

在缺乏关于某个实数上分布的先验知识时，正态分布是一个比较好的选择。 
1. 中心极限定理表明很多独立随机变量的和近似服从正态分布。 
2. 在具有相同方差的所有可能的概率分布中，正态分布在实数上具有最大的不确定性。即正态分布是对模型加入的先验知识量最少的分布。

**指数分布与Laplace分布**
<p>  $$p(x; \lambda) =  \lambda 1_{x\geq 0} \text{exp}(-\lambda x)$$ </p>
指数函数  $$1_{x \geq 0} = 0 \text{ if } x \le 0 \text{ else } 1$$ .

Laplace分布允许在任意一点  $$\mu$$  设置概率质量函数的峰值：
<p> $$\text{Laplace}(x; \mu, \gamma) = \frac{1}{2r} \text{exp} (- \frac{\lfloor x - \mu \rfloor}{\gamma})$$ </p>

To read p103



# Machine Learning basics 
ML本质上属于**应用统计学**，更多的关注于如何用计算机统计的估算复杂函数，而不太关注为这些函数提供置信空间。


P 145


# 深度前馈网络(Deep Feedforward Network)
也称前馈神经网络，或多层感知机(multilayer perceptron, MLP)。 其目的是近似某个函数  $$f^*$$。直观上，FNN可以理解为实现统计泛化而设计出的函数近似机。 

模型称为前向的，因为信息流过$$x$$的函数，流经用于定义$$f$$的中间计算过程，最终到达输出$$y$$。在模型的输出和模型本身之间没有反馈(feedback)连接。 当前馈神经网络被扩展包含反馈连接时，它们被称为循环神经网络(recurrent neural network)


### 代价函数

贯穿神经网络设计的一个主题是代价函数的梯度必须足够大和具有足够的预测性，来为学习算法提供一个好的指引。 

### 线性函数不能表示XOR函数
XOR函数是两个二进制$$x_1, x_2$$的运算。假设存在一个可以表示XOR的线性模型$$\mathcal{M}$$, 当$$x_1=0$$时，模型随着$$x_2$$的增大而增大。当$$x_1=1$$时，模型的输出随着$$x_2$$的增大而减小。因为线性模型关于变量的系数是固定的，特别的，假设其关于$$x_2$$的系数为$$w_2$$。由上面讨论可知，$$w_2$$依赖于$$x_1$$，因此不可能固定，矛盾。

