---
layout: post
title: 
date: 2019-08-03
tags: 
categories: 
author: GaoangLau
---
* content
{:toc}




# Gaussian 

高斯混合模型(Gaussian Mixture Model, abbr. GMM)是一个假设所有的数据点都是生成于有限个带有未知参数的高斯分布所混合的概率模型，即多个高斯分布函数的线性组合。理论上GMM可以拟合出任意类型的分布，通常用于解决同一集合下的数据包含多个不同的分布的情况（或者是同一类分布但参数不一样，或者是不同类型的分布，比如正态分布和伯努利分布）。




GMM可以看作是 k-means 聚类算法的一种扩展使用 。 





# Model Formula

假设随机变量为$$X$$ ,那么GMM可以表示为：

$$p(x) = \sum\limits_{k=1}^K \pi_k \mathcal{N}(x|\mu_k, \sigma_k)$$

其中$$\mathcal{N}(x\mid\mu_k, \sigma_k)$$ 为混合模型中的第$$k$$个**分量(component)**； $$\pi_k$$为**混合系数(mixture coefficient)** s.t. $$\sum\limits_{k=1}^K \pi_k = 1$$，也可以认为是第$$k$$个分量的权重。





# 应用

将GMM用于聚类时，假设数据服从混合高斯分布（Mixture Gaussian Distribution），那么只要根据数据推出 GMM 的概率分布来即可。

GMM 的 $K$ 个分量实际上对应$K$个**簇(cluster)** 。根据数据来推算概率密度通常被称作 density estimation 。特别地，当我已知（或假定）概率密度函数的形式，而要估计其中的参数的过程被称作**参数估计**。

## 参数估计

当存在$K$个聚类，公式$$p(x) = \sum\limits_{k=1}^K \pi_k \mathcal{N}(x\mid\mu_k, \sigma_k)$$中存在 $$3K$$ 个未知参数 $$\{\pi_k, \mu_k, \sigma_k\mid k \in [1..K]\}$$。如何估算这些参数 ？Short answer:  **最大似然估计**。



@TODO



## 在Kaggle(练手)比赛中的应用

[Data-Science-londo-scikit-learn](https://www.kaggle.com/c/data-science-london-scikit-learn/) 给出一个简单的训练集(1000 x 40)，存在40个特征，要求对9000个测试样本进行预测。使用简单的KNN/RandomForest算法，通过网格搜索最佳参数，最终预测结果：0.89828 

而采用GMM对数据进行处理后，再使用KNN/随机森林，预测结果可达：0.99143 （前20名的水平）





## Sklearn & GMM

`sklearn.mixture` 是一个应用高斯混合模型进行非监督学习的包，支持**diagonal，spherical，tied，full**四种协方差矩阵 (diagonal指每个分量分布有各自不同对角协方差矩阵，spherical指每个分量分布有各自不同的简单协方差矩阵， tied指所有分量分布有相同的标准协方差矩阵，full指每个分量分布有各自不同的标准协方差矩阵） ，它对数据进行抽样，并且根据数据估计模型。同时包也提供了相关支持，来帮助用户决定合适的分量分布个数。

```python
import matplotlib.pyplot as plt
import seaborn as sns; sns.set()
import numpy as np
from sklearn.datasets.samples_generator import make_blobs
# Produce experiment data, 4 clusters
X, y = make_blobs(n_samples=400, centers=4, cluster_std=0.60, random_state=0)

from sklearn.mixture import GaussianMixture
gmm = GaussianMixture(n_components=4).fit(X)
labels = gmm.predict(X)
plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis')
```

聚类图示

![image]({{site.baseurl}}/assets/img/gmm.png)

sklearn.GaussianMixture有一个`predict_proba(X)`方法，可以求出混合系统$$\pi_k$$。 方法方法返回一个大小为$$[n_\text{samples}, n_\text{clusters}]$$的矩阵，矩阵会给出任意属于某个簇的概率。接上例

```python
probs = gmm.predict_proba(X)
print(probs[:5].round(3))
```

得出结果

```bash
[[0.531 0.    0.469 0.   ]
 [0.    0.    0.    1.   ]
 [0.    0.    0.    1.   ]
 [1.    0.    0.    0.   ]
 [0.    0.    0.    1.   ]]
```

GMM模型中的超参数`convariance_type`控制这每个簇的形状自由度。

* 默认 convariance_type=’diag’, 意思是簇在每个维度的尺寸都可以单独设置，但椭圆边界的主轴要与坐标轴平行； 
* covariance_type=’spherical’ 时模型通过约束簇的形状，让所有维度相等。这样得到的聚类结果和k-means聚类的特征是相似的，虽然两者并不完全相同。
* covariance_type=’full’时，该模型允许每个簇在任意方向上用椭圆建模。



GMM提供了一种确定数据集最优成分数量的方法。由于生成模型本身就是数据集的概率分布，因此可以利用模型来评估数据的似然估计，并利用交叉检验防止过拟合。Scikit-Learn的GMM评估器内置了两种纠正过拟合的标准分析方法：赤池信息量准则（AIC）和BIC（Bayesian Information Criterion，贝叶斯信息准则）





# 参考文献

1. [Scikit-Learn中文文档](https://sklearn.apachecn.org/docs/0.21.3/20.html), ApacheCN
2. [高斯混合模型（GMM）及其EM算法的理解](https://blog.csdn.net/jinping_shi/article/details/59613054)，阿拉丁吃米粉，CSDN blog
3. [高斯混合模型(GMM)应用：分类、密度估计、生成模型](https://blog.csdn.net/jasonzhoujx/article/details/81947663)，盐味橙汁，CSDN blog， 2018-08







